{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":329006,"sourceType":"datasetVersion","datasetId":139630}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qqq sentence-transformers\n!pip install -qqq transformers\n!pip install -qqq open_clip_pytorch\n!pip install -qqq faiss-cpu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:09:05.471372Z","iopub.execute_input":"2025-06-04T12:09:05.471658Z","iopub.status.idle":"2025-06-04T12:10:32.541055Z","shell.execute_reply.started":"2025-06-04T12:09:05.471639Z","shell.execute_reply":"2025-06-04T12:10:32.540131Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement open_clip_pytorch (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for open_clip_pytorch\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ─── 1) IMPORTS & CONFIG ───────────────────────────────────────────────────────\nimport os, json, torch, faiss\nimport numpy as np, pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom transformers import (\n    CLIPProcessor, CLIPModel,\n    BlipProcessor,Blip2Processor, BlipForConditionalGeneration, Blip2ForConditionalGeneration,\n    AutoTokenizer, AutoModelForCausalLM ,AutoProcessor , pipeline,AutoModelForSeq2SeqLM\n)\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom sentence_transformers import SentenceTransformer\nimport regex as re\nfrom PIL import Image\n\nDEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIMG_DIR    = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset/images/\"\nstyles_csv = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset/styles.csv\"\nK          = 10  # top-K for both similarity & complementary\n\n# Set your Gemini API key as an environment variable\nos.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAM_mhTB1qe4-7QgNy7ONjw9mSob7x5qdw\"\nimport google.generativeai as genai\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n\n# ─── 2) LOAD & PREPARE DF ───────────────────────────────────────────────────────\ndf = pd.read_csv(\n    styles_csv, engine=\"python\", on_bad_lines=\"skip\", dtype=str\n)\ndf['image_path'] = df['id'].astype(str).apply(\n    lambda x: f\"{IMG_DIR}{x}.jpg\"\n)\norder = ['usage','productDisplayName']\ndf['text'] = df[order].astype(str).agg(' | '.join, axis=1)\n\n# ─── 3) SETUP MODELS ────────────────────────────────────────────────────────────\n# 3a) CLIP for visual search (lighter model + quantization)\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(DEVICE).half()\nclip_proc  = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n\n# 3b) SBERT for text indexing (lighter model)\ntxt_model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\", device=DEVICE)\n\n# 3c) BLIP for captioning (quantized)\nblip_proc = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nblip_model = BlipForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip-image-captioning-base\"\n).to(DEVICE).half()\n\n\ndef embed_image(path):\n    try:\n        img = Image.open(path).convert(\"RGB\")\n    except FileNotFoundError:\n        print(f\"Warning: Image not found: {path} - skipping\")\n        return None\n    inp = clip_proc(images=img, return_tensors=\"pt\").to(DEVICE)\n    with torch.no_grad():\n        emb = clip_model.get_image_features(**inp)  # Fixed: changed 'model' to 'clip_model'\n        emb /= emb.norm(p=2, dim=-1, keepdim=True)\n    return emb.cpu().numpy()\n\n\ndef embed_text(text: str) -> np.ndarray:\n    inputs = clip_proc(text=[text], return_tensors=\"pt\", padding=True).to(DEVICE)\n    with torch.no_grad():\n        txt_feats = clip_model.get_text_features(**inputs)\n    arr = txt_feats.cpu().numpy().astype(\"float32\")\n    faiss.normalize_L2(arr)\n    return arr\n\ndef generate_caption(image_path: str):\n    image = Image.open(image_path).convert(\"RGB\")\n    inputs = blip_proc(images=image, return_tensors=\"pt\").to(DEVICE)\n    out = blip_model.generate(**inputs, max_new_tokens=64)\n    caption = blip_proc.decode(out[0], skip_special_tokens=True)\n    return caption\n\ndef generate_with_gemini(prompt: str) -> str:\n    model = genai.GenerativeModel(\"gemini-1.5-pro\")\n    response = model.generate_content(prompt)\n    return response.text if hasattr(response, 'text') else str(response)\n\ndef ask_complements_local(caption, user_prompt, k=K):\n    prompt = (\n        f\"You are a professional fashion stylist.\"\n        f\"\\nYou are given a product: \\\"{caption}\\\".\"\n        f\"\\nCustomer said: \\\"{user_prompt}\\\".\"\n        f\"\\nList exactly 5 complementary and matching  items for this.\"\n        f\"\\nEach item MUST follow this format strictly:\"\n        f\"\\nCategory: <category>; Article Type: <article_type>; Color/Style: <color_or_style>; Usage: <usage>\"\n        f\"\\nSeparate each item with '//' on a single line.\"\n        f\"\\nDO NOT include any explanations or extra text. Only output the 5 formatted items.\"\n        f\"\\nDo not give same item as shown in the image in the recommendations.\"\n    )\n    # Generate response using Gemini\n    out = generate_with_gemini(prompt)\n\n    items = [itm.strip() for itm in out.split('//') if itm.strip()][:k]\n    numbered_items = [f\"{i+1}. {itm}\" for i, itm in enumerate(items)]\n    return numbered_items\n\n# ─── 4) BUILD INDEXES (first N=200 images for speed, with caching) ─────────────\n# Enhanced version of your embedding storage section\n# Replace your existing \"BUILD INDEXES\" section with this:\n\nN = 40000\n\n# Check if embeddings and metadata already exist\nif (os.path.exists(\"img_embs.npy\") and \n    os.path.exists(\"txt_embs.npy\") and \n    os.path.exists(\"filtered_df.csv\") and\n    os.path.exists(\"valid_indices.npy\")):\n    \n    print(\"Loading cached embeddings and metadata...\")\n    img_embs = np.load(\"img_embs.npy\")\n    txt_embs = np.load(\"txt_embs.npy\")\n    df = pd.read_csv(\"filtered_df.csv\", dtype=str)\n    valid_indices = np.load(\"valid_indices.npy\")\n    N = len(df)\n    print(f\"Loaded {N} valid embeddings\")\n    \nelse:\n    print(\"Building embeddings from scratch...\")\n    img_embs = []\n    valid_indices = []\n    \n    for idx, path in enumerate(tqdm(df[\"image_path\"][:N], desc=\"ImgEmb\")):\n        emb = embed_image(path)\n        if emb is not None:\n            img_embs.append(emb)\n            valid_indices.append(idx)\n    \n    if img_embs:\n        img_embs = np.vstack(img_embs)\n        valid_indices = np.array(valid_indices)\n        \n        # Filter dataframe to match valid images\n        df_filtered = df.iloc[valid_indices].reset_index(drop=True)\n        \n        # Generate text embeddings for filtered dataframe\n        txt_embs = []\n        for meta in tqdm(df_filtered[\"text\"], desc=\"TxtEmb\"):\n            txt_embs.append(embed_text(meta))\n        txt_embs = np.vstack(txt_embs)\n        \n        # Save everything for future use\n        np.save(\"img_embs.npy\", img_embs)\n        np.save(\"txt_embs.npy\", txt_embs)\n        np.save(\"valid_indices.npy\", valid_indices)\n        df_filtered.to_csv(\"filtered_df.csv\", index=False)\n        \n        # Update working dataframe\n        df = df_filtered\n        N = len(df)\n        print(f\"Saved {N} valid embeddings and metadata\")\n    else:\n        raise ValueError(\"No valid images found to process\")\n\n# Build FAISS indexes (same as before)\nfused_embs = np.concatenate([img_embs, txt_embs], axis=1).astype(\"float32\")\nfaiss.normalize_L2(fused_embs)\nsim_index = faiss.IndexFlatIP(fused_embs.shape[1])\nsim_index.add(fused_embs)\n\ntxt_index = faiss.IndexFlatIP(txt_embs.shape[1])\ntxt_index.add(txt_embs)\n\nprint(\"Indexes built successfully!\")\nprint(f\"Image embeddings shape: {img_embs.shape}\")\nprint(f\"Text embeddings shape: {txt_embs.shape}\")\nprint(f\"Dataframe shape: {df.shape}\")\n\ndef recommend(img=None, prompt=None):\n    has_img = img is not None\n    has_txt = prompt is not None\n\n    if not has_img and not has_txt:\n        print(\"Error: Both image and prompt are missing.\")\n        return\n\n    sim_df = pd.DataFrame()\n    rec_df = pd.DataFrame()\n\n    # ── A) Visual or Text Similarity ─────────────────────────\n    if has_img:\n        img_emb = embed_image(img)  # shape: (1, 512)\n        faiss.normalize_L2(img_emb)\n\n        if has_txt:\n            txt_emb = embed_text(prompt)  # shape: (1, 512)\n            faiss.normalize_L2(txt_emb)\n            qv = np.concatenate([img_emb, txt_emb], axis=1).astype(\"float32\")\n            Dv, Iv = sim_index.search(qv, K)\n        else:\n            Dv, Iv = sim_index.search(img_emb.astype(\"float32\"), K)\n\n        sim_df = df.iloc[Iv[0]][[\"id\", \"text\"]].copy()\n        sim_df[\"score_img\"] = Dv[0]\n        print(\"Top visually similar:\")\n        print(sim_df)\n        sim_df[[\"id\"]].to_csv(\"/kaggle/working/SimilarProdId.csv\", index=False)\n\n    elif has_txt:\n        txt_emb = embed_text(prompt)\n        faiss.normalize_L2(txt_emb)\n        Dt, It = txt_index.search(txt_emb, K)\n\n        sim_df = df.iloc[It[0]][[\"id\", \"text\"]].copy()\n        sim_df[\"score_txt\"] = Dt[0]\n        print(\"Top textually similar:\")\n        print(sim_df)\n        sim_df[[\"id\"]].to_csv(\"/kaggle/working/SimilarProdId.csv\", index=False)\n\n    # ── B) Caption + Complementary Retrieval ─────────────\n    if has_txt or has_img:\n        caption = generate_caption(img) if has_img else \"\"\n        if caption:\n            print(\"\\nBLIP caption:\", caption)\n\n        cats = ask_complements_local(caption, prompt if has_txt else \"\")\n        print(\"\\nStylist categories:\", cats)\n\n        cand = []\n        for cat in cats:\n            q_t = embed_text(cat)\n            faiss.normalize_L2(q_t)\n\n            Dt, It = txt_index.search(q_t, K)\n            dfc = df.iloc[It[0]][[\"id\", \"text\"]].copy()\n            dfc[\"score_txt\"] = Dt[0]\n            cand.append(dfc)\n\n        all_rec = pd.concat(cand, ignore_index=True)\n        unique_rec = (\n            all_rec\n            .sort_values(\"score_txt\", ascending=False)\n            .drop_duplicates(subset=\"id\", keep=\"first\")\n        )\n        rec_df = unique_rec.head(K)\n\n        print(\"\\nTop complementary recommendations (unique):\")\n        print(rec_df)\n        rec_df[[\"id\"]].to_csv(\"/kaggle/working/RecommendationProdId.csv\", index=False)\n\n# ─── 6) RUN EXAMPLE ────────────────────────────────────────────────────────────\nif __name__ == \"__main__\":\n    # prompt = ''\n    prompt = 'blue tshirt'\n    recommend(None,prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:17:58.593682Z","iopub.execute_input":"2025-06-04T12:17:58.593985Z","iopub.status.idle":"2025-06-04T13:10:39.549845Z","shell.execute_reply.started":"2025-06-04T12:17:58.593955Z","shell.execute_reply":"2025-06-04T13:10:39.549033Z"}},"outputs":[{"name":"stderr","text":"2025-06-04 12:18:13.931458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749039494.174150      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749039494.255201      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1bfc46c8f2d40d1bec9f7458c444a30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"267f8205c4234f3493aa3a9a62012e1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b4be8503004374b63b749dda350d85"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a22014b77f5a4051abad9581a8dcb318"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca98e9d0c2ef49089de29e80f05955d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4218fbad15643bd9d1ef70b1c5b8bfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe06af33512409599c34d2b24573f80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cea213d026842c5ba577b09e08b4677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08ce979a2f644739ada4e9b95a0f4116"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"900b59c20aae4b12bb662050d7714847"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d38132f1b08e4037b9833a0d153c5784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e60debb7ea54450fa90e7ef8c6219223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0704bc58f64938b653b545eb8941fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fb1eb35528b40d0b4309a919b04e6e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/69.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ff51d4e4b3e483b95cfd1413e889ac8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8803639b1fa46a8a9d997aac0cb80b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ce68352c7fa4f45bae990ea28d0abc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84aaa4bcae2e49bbb83b7e64cb57751a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0620ded8af5447a0af94ce64d9ce3147"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a048d985c61c435d95ef610e1aa6f172"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40130958243e4cffa56f691adc317728"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9850e8a8dc2f414cbd9bd981850ec7bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"312f786df77243d782f609e9fb3a75ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a2ce6f12f254e7cbbfabc38788be212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0bf143672f24b5db1618f7cf546eb16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d06871569dc4fa09a08d89d373274e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3fb68b0b52940e1bade2e64522c2524"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"084ba7c119a74c77ad28fc7bc346b835"}},"metadata":{}},{"name":"stdout","text":"Building embeddings from scratch...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"ImgEmb:   0%|          | 0/40000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1830dafd91f4cf5b01f28d0181b849d"}},"metadata":{}},{"name":"stdout","text":"Warning: Image not found: /kaggle/input/fashion-product-images-dataset/fashion-dataset/images/39403.jpg - skipping\nWarning: Image not found: /kaggle/input/fashion-product-images-dataset/fashion-dataset/images/39410.jpg - skipping\nWarning: Image not found: /kaggle/input/fashion-product-images-dataset/fashion-dataset/images/39401.jpg - skipping\nWarning: Image not found: /kaggle/input/fashion-product-images-dataset/fashion-dataset/images/39425.jpg - skipping\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"TxtEmb:   0%|          | 0/39996 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f1ce50b2e4d4edeb89fcceebddce55f"}},"metadata":{}},{"name":"stdout","text":"Saved 39996 valid embeddings and metadata\nIndexes built successfully!\nImage embeddings shape: (39996, 512)\nText embeddings shape: (39996, 512)\nDataframe shape: (39996, 12)\nTop textually similar:\n          id                              text  score_txt\n23435  41697       Casual | AND Women Blue Top   0.849656\n36025  43650      Casual | ONLY Women Blue Top   0.826557\n10532  43647      Casual | ONLY Women Blue Top   0.826557\n39507   8554  Casual | Basics Men Blue T-shirt   0.825004\n26472  29785  Casual | Basics Men Blue T-shirt   0.825004\n13701   9931  Casual | Basics Men Blue T-shirt   0.825004\n9097    5718  Casual | Basics Men Blue T-shirt   0.825004\n35194  41599    Casual | Basics Men Blue Shirt   0.808958\n23656  41608    Casual | Basics Men Blue Shirt   0.808958\n10272  41589    Casual | Basics Men Blue Shirt   0.808958\n\nStylist categories: ['1. Category: Bottoms; Article Type: Jeans; Color/Style: Dark Wash; Usage: Casual', '2. Category: Accessories; Article Type: Necklace; Color/Style: Silver pendant; Usage: Everyday', '3. Category: Footwear; Article Type: Sneakers; Color/Style: White canvas; Usage: Casual', '4. Category: Outerwear; Article Type: Cardigan; Color/Style: Light gray; Usage: Layering', '5. Category: Accessories; Article Type: Sunglasses; Color/Style: Tortoise shell; Usage: Sun protection']\n\nTop complementary recommendations (unique):\n       id                                          text  score_txt\n20  26536                   Casual | ID Men White Shoes   0.841396\n21  12919            Casual | iD Men Casual White Shoes   0.836335\n22  11517  Casual | Catwalk Women Lifestyle White Shoes   0.827772\n40  28344                  Casual | Idee Men Sunglasses   0.827394\n23   7043          Casual | Numero Uno White Men's Shoe   0.826598\n24  13067           Casual | Numero Uno Men White Shoes   0.822984\n25  19350           Casual | Numero Uno Men White Shoes   0.822984\n27  40693           Casual | Numero Uno Men White Shoes   0.822984\n29  13073           Casual | Numero Uno Men White Shoes   0.822984\n28  24260           Casual | Numero Uno Men White Shoes   0.822984\n","output_type":"stream"}],"execution_count":2}]}